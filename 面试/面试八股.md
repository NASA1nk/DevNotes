# Java

## native方法

`native`关键字修饰

- 说明这个方法是个原生函数，不是用java语言实现的，是使用c/c++实现的
- 被编译成了动态链接库（Dynamic Link Library，DLL），由java去调用，jdk源码中不包含
- 对于不同的平台它们是不同的，java在不同的操作系统中调用不同的`native`方法实现对操作系统的访问，
  - 因为java没有指针，不能直接访问操作系统底层

> DLL是Windows系统中**封装代码和数据以及实现资源共享**的一种方式，本质上就是一个已经编译好的机器指令文件
>
> 动态链接是指程序运行时有需要才去调用某个DLL库，一个DLL库中会包含各种函数并对外提供API

**调用过程**

1. 在java中声明`native`方法，然后编译
2. 用`javah`产生一个 `.h` 文件
3. 写一个 `.cpp`文件实现`native`方法
   1. 需要包含第二步产生的`.h`文件
   2. 需要包含了jdk带的jni.h文件
4. 将`.cpp`文件编译成动态链接库文件
5. 在java中用`System.loadLibrary()`文件加载动态链接库文件

这个`navite`方法就可被访问了

## 集合

集合只能存放对象的引用，添加基本数据类型会被自动装箱后存入集合

### 重写`hashCode()`和`equals()`

`Collection`接口：对象所在类需要重写`equals()`方法

- `contains(Object obj)`：调用对象所在类的`equals`方法来判断集合中每一个元素是否是目标元素
- `remove(Object obj)` ：调用对象所在类的`equals`方法判断是否是要删除的那个元素，只会删除匹配的第一个元素

`Set`接口：对象所在类要重写`equals()`和`hashCode()`方法，以实现**对象相等规则**

- **相等的对象必须具有相等的散列码**

- `Map`中的`key`使用`Set`存储，所以`key`所在类必须重写`hashCode()`和`equals()`方法

### `hashcode()`找位置，`equals()`判断位置上的元素

1. `HashSet`调用要添加**对象所在类**的`hashCode()`方法来得到该对象的hashCode值
2. 散列函数根据对象的hashCode值决定它在`HashSet`底层数组中的**存储位置**
   1. 散列函数会利用**底层数组的长度**计算得到对象在数组中的下标
   2. **散列函数计算会尽可能保证能均匀存储**，越是散列分布，该散列函数设计的越好
3. 如果该存储位置上**没有其他对象，则添加该对象**
4. 如果该存储位置上有其他对象，则需要比较两个对象的hashCode值
   1. 如果两个对象的**hashCode值不相等，则通过链表的方式添加该对象**
   2. 如果两个对象的hashCode值相等，再调用`equals()`方法
      1. 如果`equals()`方法结果为`true`，则添加失败
      2. 如果`equals()`方法结果为`false`，**则通过链表的方式添加该对象**

> 以链表的形式在同一个位置上存放多个元素会使得`HashSet`的性能降低，因为不能快速定位了

**重写`hashCode()`方法**

- 对同一个对象多次调用`hashCode()`方法应该返回相同的值
- 当两个对象的`equals()`方法比较返回true 时，两个对象的`hashCode()`方法的返回值也应相等
- 对象中用作`equals()`方法比较的属性，都应该用来计算hashCode值

> 所以重写了`equals()`方法也要重写`hashCode()`方法
>
> IDEA中在自定义类调用工具自动重写`equals()`和`hashCode()`方法时默认使用31
>
> - 31只占用5bits，相乘造成数据溢出的概率较小
> - 31可以由i*31== (i<<5)-1来表示，现在很多虚拟机里面都有做相关优化（提高算法效率）
> - 31是一个素数，一个数字乘以素数的最终结果只能被素数本身和被乘数还有1来整除（减少冲突）
> - 选择系数的时候要选择尽量大的系数，因为计算出来的hash地址越大，冲突就越少，查找起来效率也会提高

### HashMap为什么不直接使用hashCode()方法

- 原始的`hashCode()`方法返回的是**int整数类型**，其范围为-(2 ^ 31)~(2 ^ 31 - 1)，约有40亿个映射空间

- HashMap的容量范围是在16~2 ^ 30，通常情况下是取不到最大值的，并且设备上也难以提供这么多的存储空间，从而导致通过`hashCode()`计算出的哈希值可能不在数组大小范围内，进而无法匹配存储位置

HashMap实现了自己的`hash()`方法

- 通过两次扰动使得它自己的哈希值高低位自行进行异或运算，**降低哈希碰撞概率也使得数据分布更平均**

- 在保证**数组长度为2的幂次方**的时候，使用`hash()`运算之后的值通过&运算来获取数组下标的方式进行存储
  - 比取余操作更加有效率
  - 只有当数组长度为2的幂次方时，`h&(length-1)`才等价于`h%length`
  - 解决了哈希值与数组大小范围不匹配问题

> String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率
>
> - 都是final修饰的类，不可变性保证key的不可更改性
> - 内部已重写了`equals()`、`hashCode()`等方法



## Java虚拟机

Java内存区域

1. 介绍下Java内存区域（运行时数据区）

2. 对象的访问定位的两种方式

Java内存回收

1. 如何判断对象是否死亡（两种方法）
2. 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）
3. 如何判断一个常量是废弃常量
4. 如何判断一个类是无用的类
5. 垃圾收集有哪些算法，各自的特点
6. HotSpot为什么要分为新生代和老年代
7. 常见的垃圾回收器有那些
8. 介绍一下CMS,G1收集器
7. Minor Gc和Full GC 有什么不同呢

类加载机制

1. 简单说说类加载过程，里面执行了哪些操作？

2. 对类加载器有了解吗？

3. 什么是双亲委派模型？

4. 双亲委派模型的工作过程以及使用它的好处。

类文件结构

- 介绍一下Class类文件结构
- 常量池主要存放的是那两大常量？
- Class文件的继承关系是如何确定的？
- 字段表、方法表、属性表主要包含那些信息？

 

## 多线程

### 线程创建

1. 继承`Thread`类
2. 实现`Runnable`接口
3. 实现`Callable`接口

> 重写`run()`方法：**线程体**

**比较**

推荐使用`Runnable`接口

- **避免单继承局限性**
- **方便同一个对象被多个线程共享使用**
  - 适合多个相同线程来处理同一份资源

### 线程启动

通过`start()`方法启动线程

- **Java的线程是不允许启动两次的**，一个线程对象只能调用一次`start()`方法启动
  - 重复调用抛出异常（运行时异常）
- 直接调用`run()`无法开启一个新线程
  - `run()`方法**只是一个类中的普通方法**，直接执行和普通的方法没区别
  - `start()`方法首先做了创建线程等一系列工作，然后调用`run()`方法
    - **即`start()`方法是在一个新的线程上面去调用`run()`方法**，`run()`方法则是在调用`run()`方法的当前的线程当中执行

#### 静态代理

**StaticProxy**

`new Thread(Runnable).start();`

- **真实对象和代理对象都要实现同一个接口**
- 代理对象要代理真实对象

**优点**

- **真实对象专注于自己的工作**
- 代理对象可以做真实对象做不了的工作

### 线程终止

- `run()`方法完成后线程终止
  - 有时`run()`方法是永远不会结束的，比如在服务端程序中使用线程进行监听客户端请求
- 使用`stop()`方法强行终止线程
  - 不推荐使用，因为stop和suspend、resume一样，可能发生不可预料的结果，不知道资源是否释放
- 使用`interrupt()`方法中断线程

### 线程状态

1. 创建（`NEW`）
   1. 当一个`Thread`类或其子类的对象被**声明并创建**时，新生的线程对象处于创建状态
   1. 还没有调用`start()`方法
2. 就绪（`RUNNABLE`）
   1. 处于创建状态的线程被`start()`后将进入线程队列**等待CPU时间片**
   2. 此时它已具备了运行的条件，只是没分配到CPU资源
3. 运行（`RUNNING`）
   1. 当就绪的线程被调度并获得CPU资源时就进入运行状态
   2. `run()`方法定义了**线程的操作和功能**
4. 阻塞（`BLOCKED`）
   1. 在某种特殊情况下，被人为挂起或执行输入输出操作时，**让出CPU**并临时中止自己的执行，进入阻塞状态
5. 死亡（`DEAD`）
   1. 线程完成了它的全部工作或线程被提前强制性地中止或出现异常导致结束
   2. **结束后的线程不能再次启动**



### 线程池

`ThreadPool`：并发框架

**优点**

1. **降低资源消耗**：通过重复利用已经创建的线程**降低线程创建的和销毁造成的消耗**
   1. 例如工作线程Woker会**无限循环**获取阻塞队列中的任务来执行

2. **提高响应速度**： 当任务到达时，任务可以不需要等到线程创建就能立即执行
3. **提高线程的可管理性**： 线程是稀缺资源，Java的线程池可以对线程资源进行统一分配、调优和监控



# 计算机网络

## 网络结构

### OSI 7层模型

- 应用层：**针对特定应用**的协议，为应用程序提供服务。如电子邮件、远程登录、文件传输等协议
- 表示层：主要负责**数据格式的转换**，把不同表现形式的信息转换成适合网络传输的格式
- 会话层：通信管理，负责建立和断开**通信连接**。即何时建立连接、何时断开连接以及保持多久的连接
- 传输层：在两个**通信结点**（端到端）之间负责数据的传输，起着可靠传输的作用
- 网络层：**路由选择**，在多个网络之间转发数据包，负责将数据包传送到目标地址
- 数据链路层：负责物理层面上互联设备之间的通信传输。例如与一个以太网相连的两个节点之间的通信。是数据帧与1、0比特流之间的转换
- 物理层：主要是1、0比特流与电子信号的高低电平之间的转换

> 网络层是针对主机与主机之间的通信，而传输层针对的是不同主机进程之间的通信
>
> 网络层负责将数据包从源IP地址转发到目标IP地址，而传输层负责将数据包再递交给主机中对应端口的进程

## TCP

基于TCP的协议：HTTP、HTTPS、Telnet、FTP、SMTP

### TCP头部

标志位：URG、ACK、PSH、RST、SYN、FIN

- SYN：**发起一个新连接**
- ACK：**确认序号**ack有效
  - 建立连接后所有的报文段ACK=1
- FIN：**释放一个连接**
- PSH：接收方应该尽快将这个报文交给应用层
  - 不用等待缓存满
- RST：重置连接
- URG：紧急指针（urgent pointer）有效
  - 紧急数据，尽快传送

### 三次握手建立连接

- 第一次握手：客户端随机产生一个自己的初始序列号seq，令自己的SYN=1，发送给服务器，进入`SYN_SENT`状态
  - **SYN=1，seq=x**
- 第二次握手：服务器收到客户端的SYN=1之后，知道是客户端请求建立连接，令自己的SYN=1，ACK=1，产生一个ack=seq+1表示确认，再随机产生一个自己的初始序列号seq，发送给客户端，进入`SYN_RCVD`状态
  - **SYN=1，ACK=1，seq=y，ack=x+1**
- 第三次握手：客户端检查ack是否等于seq+1，ACK是否等于1，检查正确之后令自己的ACK=1，产生一个ack=seq+1，发送给服务器，进入`ESTABLISHED`状态
  - **无SYN，ACK=1，seq=x+1，ack=y+1**

服务器检查ACK是否等于1和ack是否等于seq+1后，也进入`ESTABLISHED`状态，代表三次握手完成，连接建立

> 当一个连接被建立或被终止时，交换的报文段只包含TCP头部，而没有数据

### 第三次握手中如果客户端的ACK未送达服务器会怎么样

由于服务器没有收到客户端的ACK确认，会重发之前的SYN+ACK（默认重发五次，之后自动关闭连接进入CLOSED状态），客户端收到后会重新传ACK给服务器

### 为什么TCP建立连接不能是两次握手

三次的目的是什么？能否通过两次达到同样的目的

**不采用 三次握手，那么只要服务器发出确认，连接就建立了**

1. 可能会出现**已失效的客户端连接请求报文段又传到了服务器端**
   1. 客户端发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致**延误到连接释放以后的某个时间才到达服务器**
   2. 服务器收到失效的连接请求报文段后，会误认为是客户端再次发出的一个新的连接请求，于是就向客户端发出确认报文段，同意建立连接
   3. 而客户端并没有发出建立连接的请求，因此不会理睬服务器的确认，也不会向服务器发送数据。但服务器却以为新的连接已经建立，并一直等待客户端发来数据
2. **两次握手服务器无法确认客户端是否正确接收第二次握手的报文**，也无法保证服务器和客户端之间成功互换初始序列号

### TCP建立连接能不能是四次握手

可以，但是会降低传输的效率

- 第二次握手时服务器只发送ACK和ack
- 服务器的SYN和seq在第三次握手时发送
- 原来协议中的第三次握手变为第四次握手

> 四次握手中的二、三可以合并

### 四次挥手释放连接

- 第一次挥手：客户端将FIN置为1，发送一个序列号seq给服务器，进入FIN_WAIT_1状态
  - FIN=1表示客户端已经没有要发送的数据，主动请求释放连接
- 第二次挥手：服务器收到FIN之后，令自己的ACK=1，产生一个ack=seq+1表示确认，进入CLOSE_WAIT状态
  - 客户端已经不发送的数据了，但仍可以接受服务器发来的数据
  - 即客户端向服务器的单向连接被释放
- 第三次挥手：服务器将FIN置1，发送一个序列号seq给客户端，进入LAST_ACK状态
  - FIN=1表示接服务器已经把数据发送完了，请求释放连接
- 第四次挥手：客户端收到服务器的FIN后，进入TIME_WAIT状态，令自己的ACK=1，产生一个ack=seq+1表示确认。服务器收到ACK后，确认ack后变为CLOSED状态，不再向客户端发送数据。
  - 客户端**等待2*MSL**（报文段最长寿命）时间后，也进入CLOSED状态，完成四次挥手

> 2，3次分开挥手，先发送ACK，再发送FIN，是因为服务器可能还没有发送完数据
>
> 如果客户端没有收到第二次挥手的ACK确认，会重新发送FIN请求

### 第四次挥手的TIME_WAIT有什么用

第四次挥手时，**客户端发送给服务器的ACK有可能丢失，TIME_WAIT状态就是用来重发可能丢失的ACK报文**

如果服务器没有收到ACK，就会重发FIN，如果客户端在2MSL的时间内收到了FIN，就会重新发送ACK并**再次等待2MSL**，防止服务器没有收到ACK而不断重发FIN

MSL（Maximum Segment Lifetime），指一个片段在网络中最大的存活时间，2MSL就是**一个发送和一个回复所需的最大时间**

如果直到2MSL客户端都没有再次收到FIN，那么客户端推断ACK已经被成功接收，则结束TCP连接

> 不能一次性将确认ACK报文和FIN报文发给客户端，所以这里多出来了一次

### TCP的流量控制

控制发送方的发送速度，避免接**收方处理不过来**产生网络拥塞或丢包

> 滑动窗口（Sliding Window）实现

通信过程中，接收方根据自己接收缓存的大小，通过**设置确认报文段中的TCP首部字段中的窗口字段**动态调整发送方发送窗口的大小

- **发送窗口 = min(接收窗口，拥塞窗口)**
- 发送窗口的大小指的是**无需等待确认应答而可以继续发送数据的最大值**，即不需要接收端的应答可以一次连续的发送数据

**为什么使用滑动窗口**

- TCP采用确认应答策略
- 对每一个发送的数据段都会给一个ACK确认，收到ACK后再发送下一个数据段，这样做性能比较差，尤其是往返时间长的时候
- 使用滑动窗口可以一次发送多条数据，从而就提高了性能

**滑动窗口中的数据分为两种**

- 已经发送，但还没有收到确认的
- 可以发送，但还没有发送的



### TCP的拥塞控制

控制发送方的发送速度，避免**网络来不及处理**从而导致网络拥塞

- 对网络中某一资源的需求超过了该资源所能提供的可用部分，网络性能就要变坏，会造成一部分网络资源丢失掉

- 网络资源：链路容量（带宽）、交换结点中的缓存和处理机等

**前提**：接收窗口足够大，**发送窗口取决于网络的拥塞程度，即拥塞窗口**（cwnd）

拥塞控制主要由四个算法组成

- **慢启动（Slow Start）**
- **拥塞避免（Congestion voidance）**
- **快重传 （Fast Retransmit）**
- **快恢复（Fast Recovery）**

#### 慢启动和拥塞避免

**慢启动**：指数增长

- 开始发送时，将**拥塞窗口大小设为1MSS**（最大报文段），每收到一个新的**确认报文段**，窗口大小+1
- 即**每经过一个轮次，窗口大小翻倍**，指数增大（1-2-4-8...）
  - 轮次时间RTT也变大

> 慢启动是指一开始像网络注入的报文段少，而不是指拥塞窗口增长慢

**拥塞避免**：线性增长

- **每经过一个轮次，窗口大小+1**
  - 轮次时间不变

建立连接后还会**给拥塞窗口设置一个慢启动的门限值**ssthresh，为了避免拥塞窗口过大（指数增长太快）

- cwnd < ssthresh：使用慢启动
- cwnd = ssthresh：慢启动，拥塞避免均可
- cwnd > ssthresh：使用拥塞避免
  - 如果一开始拥塞窗口就大于门限值，则直接使用拥塞避免算法


> 拥塞避免不是指完全避免拥塞，而是将拥塞窗口换成线性增加，使得网络比较不容易出现拥塞

无论是慢启动还是拥塞避免，当**达到网络拥塞值时**（发现丢包判断拥塞）

- 将门限值ssthresh降低为此时发送窗口大小的一半，但不能小于2
- 将拥塞窗口cwnd设为1，开始慢启动过程

#### 快重传和快恢复

用于**改进TCP的性能**，新增的两个拥塞控制算法

- 有时候个别报文段会在网络中丢失，但**实际上网络并没有发生拥塞**
- 报文段丢失会导致发送方超时重传，并且认为网络拥塞了，这时发送方会降低门限值，启动慢启动算法，将拥塞窗口设为1，**降低了传输效率**

**快重传**

使用冗余ACK来判定丢包，快速重传，避免触发超时计时器，这样就不会认为网络拥塞从而降低了拥塞窗口，提高了网络吞吐量

- 要求接收方在收到一个**失序的报文段**后就立刻发出**冗余ACK**而不要等到自己发送数据时**捎带确认**
  - **冗余ACK**就是再发一次之前收到的报文段的确认，即收到的有序报文段的最后一个的确认
- 规定发送方**只要收到三个冗余ACK就立即重传**对方尚未收到的报文段
  - 不必继续等待设置的**重传计时器**时间到期

**快恢复**

当发送方收到三个冗余ACK时，知道只是丢失了个别报文段而不是网络拥塞，所以不启动慢启动算法，而是执行拥塞避免算法

- 将门限值**ssthresh降低为此时拥塞窗口cwnd大小的一半**
- 将拥塞窗口cwnd设置为此时的ssthresh
- **执行拥塞避免算法**

> 还有一个就是因为如果网络出现拥塞的话就不会收到三个冗余ACK，所以发送方认为现在网络没有出现拥塞

### TCP拥塞控制和流量控制的区别

拥塞控制通常表示的是一个**全局性的过程**，它会涉及到网络中所有的主机、路由器和降低网络传输性能的所有因素

流量控制发生在发送端和接收端之间，只是点到点之间的控制

**相同点**

- **现象都是丢包**
- 实现机制都是让发送方发的慢一点，发的少一点

**不同点**

- **丢包位置不同**

  - 流量控制丢包位置是在**接收端**上

  - 拥塞控制丢包位置是在**路由器**上

- **作用的对象不同**
  - 流量控制的对象是**接收方**，怕发送方发的太快，使得接收方来不及处理
  - 拥塞控制的对象是**网络**，怕发送方发的太快，造成网络拥塞，使得网络来不及处理

### TCP粘包

**TCP是面向连接的、可靠的流式传输协议**，**流式传输**是指数据的传输方式是**字节流**式的，流是没有边界的

- 应用程序首先要将自己的数据**通过套接字发送**。应用层交付给TCP的是**结构化的数据**（表示层）
- TCP把应用程序交付下来的数据仅仅看成是一连串的**无结构**的**字节流**，并不知道所传送的字节流的含义

TCP粘包是指

- 发送方发送的**若干包数据到达接收方时粘成了一包**
- 从接收缓冲区来看，**后一包数据的头紧接着前一包数据的尾**

出现粘包的原因是多方面的，可能是来自发送方，也可能是来自接收方

1. 由**TCP连接复用**造成的粘包问题
   1. 如果没有复用，一个连接只提供给端到端的两个进程使用，这是数据的传输方和发送方都是**约定好了数据格式的**，但是连接复用则是**多个进程**使用一个TCP连接，此时**多种不同结构**的数据通过TCP流式传输，边界分割就可能出现粘包问题
2. TCP默认使用的**Nagle算法**造成的粘包问题
   1. Nagle算法是指，只有上一个分组得到确认，才会发送下一个分组，收集多个小分组，在一个确认到来时一起发送
   2. 所以将多个分组拼成一个数据段发送出去时如果没有处理好边界，在解包的时候会发生粘包问题。
3. 接收方不及时接收缓冲区的包造成的粘包问题
   1. TCP接收到数据包时并不会马上交到应用层进行处理，或者说应用层并不会立即处理
   2. TCP将接收到的数据包保存在接收缓存里，然后应用程序主动从缓存读取收到的分组
   3. 如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包
4. **数据包过大**造成的粘包问题
5. 流量控制和拥塞控制造成的粘包问题

> Nagle算法
>
> - 在socket网络编程中，都是端到端通信，由`<cport,sport,cip,sip,协议>`这个五元组标识一条TCP连接，发送端和接收端都有成对的socket。
> - 发送端为了将多个包更加高效的的发给接收端，于是采用了Nagle算法，将多次间隔较小、数据量较小的数据合并成一个数据量大的数据块，然后进行封包。
> - 所以接收端就必须使用高效科学的拆包机制来分辨这些数据

**处理粘包问题**

当发送方发送的多组数据毫不相干，或者是并列关系，就要处理粘包

> 同一块数据的不同部分就不需要处理粘包

1. 发送方造成的粘包问题，可以关闭Nagle算法
2. **接收方没有办法来处理粘包现象**，只能将问题交给应用层来处理
3. 应用层从接收缓存中读取分组时，读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成
   1. **格式化数据**：每条数据有固定的格式（开始符，结束符）
      1. 选择开始符和结束符时要确保数据的内部不包含开始和结束符
   2. **发送长度**：发送数据时，将数据的长度一并发送

**UDP为什么不会产生粘包问题**

- UDP是面向报文段的，不是流式的

### TCP可靠传输

可靠传输是指

- 传输的**信道不产生差错**
- 保证传输**数据的正确性**，无差错、不丢失、不重复且按顺序到达

TCP实现可靠传输

- 应用数据被分割成TCP认为最适合发送的**块**进行传输
- **超时重传**，TCP发出一个分组后，它启动一个定时器，等接收方确认收到这个分组
  - 如果发送方不能及时收到一个确认，将重传给接收方
- **序号**，用于检测丢失的分组和冗余的分组
- **确认**，告知对方已经正确收到的分组以及期望的下一个分组
- **校验和**，校验数据在传输过程中是否发生改变，如校验有错则丢弃分组；
- **流量控制**，使用滑动窗口，发送窗口的大小由接收窗口和拥塞窗口的的大小决定
  - 当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失
- **拥塞控制**：当网络拥塞时，减少数据的发送

### TCP与UDP的区别

- TCP是面向连接的，UDP是无连接的
- TCP是可靠的，UDP不可靠
  - 指UDP接收方收到报文后，**不需要给出任何确认**
- TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多
- **TCP是面向字节流的，UDP是面向报文的**
  - 发送数据时以字节为单位，一个数据包可以拆分成若干组进行发送
  - **UDP一个报文只能一次发完**
  - 所i一**TCP无界，UDP有界**
- TCP有拥塞控制机制，UDP没有
- TCP首部开销（20字节）比UDP首部开销（8字节）要大
- UDP的主机不需要维持复杂的**连接状态表**

## UDP

基于UDP的协议：DHCP、DNS、SNMP、TFTP、BOOTP

## DNS

Domain Name System：DNS，域名系统

- 应用层协议
- 是一种组织成**域层次结构**的计算机和网络服务命名系统
  - 域名的层级关系类似一个**树状结构**
- DNS中的域名用**句点**分隔，代表了不同层次之间的**界限**
  - 在域名中，**越靠右**的位置表示其层级**越高**

> 用户主机上运行着DNS客户端

**根DNS服务器**

- **顶级DNS服务器**（com，cn，org）
  - **权威DNS服务器**（google.cn，baidu.com）

其中**根DNS服务器信息保存在所有的DNS服务器中**，任何DNS服务器都可以找到并访问根DNS服务器

DNS 查询共有两类

- 递归查询
- 迭代查询

**递归查询**是指当A向B查询某个域名的IP地址时，如果B不知道被查询的域名的IP地址，那么B会替A向更上层的服务器发起查询，将查询结果返回 A

**迭代查询**是指当A向B查询某个域名的IP地址时，如果B不知道被查询的域名的IP地址，B会告诉A下一步应该向哪个服务器查询，由A自己去查。

> 一般来说，主机向本地域名服务器的查询是递归查询，而本地域名服务器向根域名服务器的查询是迭代查询

### 为什么DNS使用UDP

**DNS主要基于运输层的UDP协议**

User Datagram Protocol：无连接的，尽最大能力交付的不可靠数据连接

- 一次UDP的信息交换可以短到只包含两个包：一个查询包和一个响应包。一次TCP的信息交换则至少包含9个包：三次握手、一个查询包、一个响应包和四次挥手


- 考虑到效率原因，TCP连接的开销大，所以采用UDP作为DNS的运输层协议

### 在浏览器地址栏输入一个URL背后的技术

**DNS查询过程**

1. 浏览器**解析URL**获取Web服务器和文件名，然后**生成HTTP请求报文**
   1. URL由**协议**，**域名**，**文件路径**，**端口**组成
   2. 协议默认为http，端口默认为80端口
   3. 没有路径代表访问根目录下事先设置的**默认文件**
2. **查看浏览器缓存**中是否有对应域名，如果有对应域名，则直接返回ip地址
3. 当浏览器缓存中没有对应的域名时，需要去**查看浏览器本机的`hosts`文件**，如果有对应域名，则直接返回ip地址
   1. `hosts`文件保存了一些域名以及域名对应的IP地址
4. 当`hosts`文件中没有对应的域名时，需要去查看**本地DNS解析器缓存**，如果有对应域名，则直接返回ip地址
5. 当本地DNS解析器缓存中没有对应的域名时，需要由**DNS客户端**去**访问本地DNS服务器**，如果有对应域名，则直接返回ip地址
   1. TCP/IP参数中设置的首选DNS服务器
6. 本地DNS服务器如果没有对应的域名，需要去**访问13台根DNS服务器**
7. **根DNS服务器根据域名将对应授权管理的顶级DNS服务器ip地址返回给本地DNS服务器**
8. 本地DNS服务访问顶级DNS服务器，顶级DNS服务器根据域名将权威DNS服务器返回给本地DNS服务器
9. 权威DNS服务器将对应的ip地址返回给本地DNS服务器
10. 本地DNS服务器将ip返回给浏览器
11. 浏览器向服务器ip发起HTTP请求
11. 服务端响应HTTP请求，将html代码返回给浏览器
11. 浏览器浏览器解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上
11. JavaScript的解析是由浏览器中的JS解析引擎完成的。JS是单线程运行，也就是说，在同一个时间内只能做一件事，所有的任务都需要排队，前一个任务结束，后一个任务才能开始

> 所以从客户端到本地DNS服务器是属于递归查询，DNS服务器之间的交互查询是迭代查询

## HTTP

HTTP不可以使用UDP，**HTTP需要基于可靠的传输协议**，而UDP不可靠

> **http 3.0 使用udp实现**

### http1.x 和http2.0的区别

- http1.x使用的文本（字符串）传送，http2使用**2进制传送**，解析速度更快
  - 2进制传送为多路复用提供了基础

- http1.x有**顺序**和**阻塞**约束，发送一个请求后必须等待响应后再发送下一个请求，http2支持**多路复用**，同一个连接可以**同时发送多个请求，并行传输数据，再接收响应**
  - 多路复用用长连接避免创建多个TCP连接带来的网络开销，解决了对同一域名的请求阻塞问题
- http2支持**头部（Header）压缩**
  - 对于Header中相同的数据，不会在每次通信中重新发送，而是采用**追加**或**替换**的方式
  - 在客户端和服务端之间共同维护一个Header表，存储之前发送的 key-value对。Header表在连接期间始终存在

- http1.x只能在客户端发送请求后，服务端再返回内容，http2支持**服务端主动推送**，支持在客户端未请求的情况下，主动发送内容给客户端

### HTTP的长连接与短连接

- **HTTP/1.0 默认使用的是短连接**
  - 短连接时指浏览器每请求一个静态资源，就建立一次连接，任务结束就中断连接
- **HTTP/1.1 默认使用的是长连接**
  - 长连接是指在一个网页打开期间，所有网络请求都使用**同一条已经建立的连接**
  - 当没有数据发送时，双方需要发检测包以维持此连接，长连接不会永久保持连接，而是有一个保持时间
  - 实现长连接要客户端和服务端都支持长连接

> 当请求频繁时，建立和关闭TCP连接会浪费时间和带宽，而重用一条已有的连接性能更好
>
> 长连接会占用服务器的资源

### GET和POST的区别

- **GET操作是幂等的**，不会对服务器产生任何修改，所以可以对GET请求的数据做缓存
  - 幂等是指同样的请求执行一次与执行多次的效果是一样的，即f(f(x)) = f(x)
  - 这个缓存可以做到浏览器本身上（彻底避免浏览器发请求）
  - 也可以做到代理上（如nginx）
  - 所以GET请求可以被保存浏览器书签，而POST请求不能
- POST请求不是幂等的，因此结果不能缓存
- GET数据有长度限制（指URL的长度限制）
  - HTTP协议本身对URL长度并没有做任何规定，**实际的限制是由客户端/浏览器以及服务器端决定的**
  - 之所以要限制长度，是因为**服务器解析URL的时候要分配内存**
    - 对于一个字节流的解析必须分配buffer来保存所有要存储的数据
    - URL必须当作一个整体看待，无法分块处理，所以处理一个URL请求时必须分配一整块足够大的内存
    - 如果URL太长，而并发又很高，就容易挤爆服务器的内存
- GET产生一个TCP数据包，POST产生两个TCP数据包
  - 对于GET请求，浏览器会把header和data一并发送出去，服务器响应200
    对于POST请求，浏览器先发送header，**服务器响应100 continue**，浏览器再发送data，服务器响应200
- GET可以发送的参数只能是ASCII类型，POST没有限制，甚至可以传输二进制

> 后退或刷新时，GET是无害的，POST会重新提交表单
>
> 并没有什么限制规定GET一定不能没有body，POST就一定不能把参放到URL的query上，只要请求的客户端和服务器端能够约定好
>
> REST API就是做了接口规范，通过看HTTP的method就可以明白接口是什么作用，解析格式也得到了统一

### POST是否比GET更安全

- 如果从前端安全xss csrf等来讲，POST比GET安全
  - 因为POST用body传输数据，GET用URL传输数据，更加容易被看到

- 如果从TCP协议上来讲，HTTP都是明文，都不安全
  - HTTPS则都是安全的

> 从客户端到服务器端中间有大量的节点，包括网关，代理等。它们的access log通常会记录完整的URL，比如nginx的默认access log。如果URL上携带敏感数据，就会被记录下来
>
> 私密数据在body里也是可以被记录下来的**，因此如果请求要经过不信任的公网，避免泄密的唯一手段就是HTTPS**

### Cookie与Session的区别

HTTP是**无状态协议**，之前已经认证成功的用户状态无法通过协议层保留下来，即**无法实现状态管理**

Cookie与Session都是用来跟踪**浏览器用户身份的会话方式**，一般使用Cookie来管理Session

**管理流程**

1. 客户端将ID和密码等登录信息放在请求报文的实体部分，以POST方式发送给服务器
2. **服务器发放用于识别用户的Session ID**，通过验证请求报文中的登录信息进行身份验证，然后**把用户的认证状态和Session ID绑定，然后记录在服务器端**
3. 服务器在向客户端返回响应时，会**在响应报文的首部的Set-Cookie字段内写入Session ID**
4. **客户端收到响应报文后，将Session ID作为Cookie保存在本地**。下次再向服务器发送请求时，**浏览器会自动发送Cookie，即发送了Session ID。**
5. 服务器就可以**通过Session ID来识别用户即认证状态**

**Cookie**

- **存在客户端浏览器里**，可以设置过期时间
  - 安全性较差

- 浏览器对cookie数量和大小有限制的，如果超过了这个限制，会丢失信息
  - 一个站点最多保存20个cookie

- cookie本身大小也有限制，一般是4kb
- 每次访问服务器时，浏览器会自动在首部中携带cookie

**Session**

- **存在服务端**，由服务器维护，一段时间后session就失效了
  - 安全性相对更高
  - 访问增多会占用服务器性能，考虑到服务器性能方面应当使用cookie

- **session本质上还是通过cookie实现的**
  - 浏览器的cookie中只保存一个session id，Session的信息保存在服务端，由session id标识
  - 所以如果客户端禁止cookie，那么能session也就不能用了
    - 可以手动通过URL传值、隐藏表单传递session id

> Session失效其实是服务器设置了失效时间。如果用户长时间不和服务器交互，session就会被销毁，交互的话，就会刷新session时间

### Token

当客户端频繁向服务端请求数据，服务端需要频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示

Token是**服务端生成的一串字符串**，作为客户端进行请求的一个令牌

- 当客户端第一次请求登录后，服务器生成一个Token并将此Token返回给客户端
- 以后客户端只需携带这个Token前来请求数据即可，无需再用用户名和密码登录

> 减少频繁的查询数据库，减轻服务器的压力

## HTTPS

HTTP+SSL（TLS）

HTTP使用的是80端口，HTTPS使用443端口

### SSL/TLS协议

不使用SSL/TLS的HTTP通信就是不加密的通信，所有信息明文传播带来了三大风险

- **窃听风险**（eavesdropping）：第三方可以**获知**通信内容
- **篡改风险**（tampering）：第三方可以**修改**通信内容
- **冒充风险**（pretending）：第三方可以**冒充**他人身份参与通信

使用SSL/TLS协议解决这三大风险

- 所有信息都是**加密传播**，第三方无法窃听
- 具有**校验机制**，一旦被篡改，通信双方会立刻发现
- 配备**身份证书**，防止身份被冒充

**通信流程**

1. 客户端（通常是浏览器）向服务器发出加密通信的请求（ClientHello请求）
   1. 需要向服务器提供客户端生成的随机数，用于生成对话密钥
2. 服务器收到客户端请求后，向客户端发出回应（SeverHello回应）
   1. 需要向客服端提供服务器生成的随机数，用于生成对话密钥
   2. 服务器的数字证书
3. 客户端收到服务器回应以后，首先要验证服务器证书，如果证书不是可信机构颁布、或者证书中的域名与实际域名不一致、或者证书已经过期，就会显示警告
4. 如果证书没有问题，客户端就会从证书中取出服务器的公钥，向服务器发送信息
   1. 一个随机数（pre-master key），该随机数会用证书中的服务器公钥加密，防止被窃听
   2. 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送
   3. 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验
5. 服务器收到客户端的随机数（pre-master key）后，计算生成本次会话所用的会话密钥。然后，向客户端最后发送下面信息
   1. 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。
   2. 服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供客户端校验

> 需要将公钥放在数字证书中来保证可信
>
> 客户端与服务器进入加密通信后就完全是使用普通的HTTP协议，只不过用会话密钥加密内容

### 对称加密和非对称加密

**对称密钥加密**：共享密钥加密

- 加密解密用一个密钥
- 需要将密钥发送给对方

**非对称密钥加密**：公开密钥加密

- 私钥自己保存
- 公钥发送给要通信的对方

非对称密钥加密比对称密钥加密的处理速度慢，所以HTTPS采用混合加密的方式加密

- 在发送密钥的时候使用非对称密钥加密
- 在通信时使用对称加密



### 数字签名和数字证书

**数字签名**：digital signature

- 使用私钥对内容生成数字签名，发送给对方，对方用公钥解密

**数字证书**：digital certificate

- 数字签名接收方所使用的公钥可能会替换，导致不安全，别人就可以用私钥伪造数字签名

- 需要找**证书中心**（certificate authority，CA）为公钥做认证。证书中心用自己的私钥，对公钥和一些相关信息一起加密，生成数字证书
- **发送数字签名的时候附带上数字证书**，对方使用CA的公钥解密，验证是否是真的数字签名

**认证流程**

1. 客户端向服务器发出加密请求
2. 服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端
3. 客户端（浏览器）的**证书管理器**中有**受信任的根证书颁发机构列表**客户端根据这张列表，查看解开数字证书的公钥是否在列表之内
   1. 如果数字证书记录的网址与客户端正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告
   2. 如果这张数字证书不是由**受信任的机构**颁发的，浏览器会发出另一种警告
4. 如果数字证书是可靠的，客户端就可以使用**证书中的服务器公钥**，对信息进行加密，然后与服务器交换加密信息

## URL编码

由于RFC 1738规定URL只能使用英文字母、阿拉伯数字和某些标点符号，不能使用其他文字和符号，所以如果URL中有汉字，就必须编码后使用。但RFC 1738没有规定具体的编码方法，而是交给应用程序自己实现

这就导致不同的操作系统、不同的浏览器、不同的网页字符集，将导致完全不同的编码结果

- **网址路径**使用utf-8编码
- **查询字符串**使用操作系统的默认编码
- **GET和POST方法生成的URL**使用网页的编码（HTML源码中字符集设定）

解决方案

- 使用Javascript先对URL进行编码，然后再向服务器提交，不要给浏览器插手的机会
- 因为Javascript的输出总是一致的，所以就保证了服务器得到的数据是格式统一的

> 无论网页的原始编码是什么，一旦被Javascript编码，就都变为unicode字符。也即Javascipt函数的输入和输出默认都是Unicode字符

# 操作系统

## 用户态和内核态

问题

- **如何限制代码行为**
  - 比如禁止：设置特殊寄存器的值，访问存储器的任意位置，I/O 请求，申请更多系统资源等
- 在运行这个程序的时候，**如何切换到另一个程序**
  - **进程调度应该是OS才有的权限**

解决方法

**引入用户态和内核态和两种模式**

- 用户态无法执行受限操作，执行这些操作会引发异常
- 内核态只能由操作系统运行，可以执行特权操作
- 用户程序通过**系统调用**执行这些特权操作
  - OS执行前会判断进程是否有**权限**执行相应的指令

**受限直接执行**：Limited Direct Execution

- 区分用户态和内核态的执行机制

### 陷入内核态

**系统调用**（trap）、**中断**（interrupt）**和异常**（exception）都会陷入内核态

- 系统调用是**用户进程主动发起**的操作
  - 进程发起系统调用
  - 陷入内核态
  - 由操作系统执行系统调用
  - 再返回到进程

- 中断和异常是被动的，**无法预测发生时机**
  - 中断包括 I/O 中断，外部信号中断，各种定时器引起的时钟中断等
  - 异常包括程序运算引起的各种错误如除0，缓冲区溢出，缺页等

> 在系统的处理上，中断和异常类似，都是**通过中断向量表来找到相应的处理程序进行处理**
>
> - 区别在于**中断来自处理器外部**，不是由任何一条专门的指令造成，而**异常是执行当前指令的结果**
>
> C访问**访问空指针**会陷入内核态
>
> - 访问指针相当于**访问一个虚拟地址**，硬件会将虚拟地址映射到真实的物理内存
> - 如果映射失败，硬件会抛出一个**段错误异常**（`page fault exception`），此时会**从用户态转为内核态**进行处理
>   - OS会在中断向量表中找到处理`page fault exception`的中断向量，执行相应的handler

### 系统调用

System Call

- 计算机系统的各种硬件资源是有限的，在现代多任务操作系统上同时运行的多个进程都需要访问这些资源
- **为了更好的管理这些资源**，进程是不允许直接操作这些资源的，所有对这些资源的访问都必须由操作系统控制（受限直接执行）
  - 即操作系统是使用这些资源的唯一入口，**这个入口就是操作系统提供的系统调用**
- 在linux系统中**系统调用是用户空间访问内核的唯一手段**

#### 软中断

**用户空间的程序无法直接执行内核代码**，它们不能直接调用内核空间中的函数，**因为内核驻留在受保护的地址空间上**

- 如果进程可以直接在内核的地址空间上读写的话，系统安全就会失去控制

所以应用程序通过软中断的机制通知系统，告诉内核自己需要执行一个系统调用，希望系统切换到内核态，这样内核就可以代表应用程序来执行该系统调用

## 并行与并发

- 并行：parallelism
  - 多个处理器或者多核处理器**同时**处理多个不同的任务
- 并发：concurrency
  - 通过调度算法，一个处理器同时处理多个任务

## 线程

线程可以分为两类

- **用户级线程**：`user level thread`
  - 有关线程管理的所有工作都**由应用程序完成**，内核意识不到线程的存在
  - 在应用程序启动后，操作系统分配给该程序一个进程号和对应的内存空间等资源
  - 应用程序通常先在一个线程（主线程）中运行，在其运行的某个时刻，可以通过调用线程库中的函数创建一个在相同进程中运行的新线程
  - 用户级线程的优点是非常高效，不需要进入内核空间，缺点是并发效率不高
- 内核级线程：`kernel level thread`
  - 有关线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只能**调用内核线程的接口**
  - 内核维护进程及其内部的每个线程，调度也由内核基于线程架构完成
  - 内核级线程的优点是内核可以将不同线程更好地分配到不同的CPU，以实现真正的并行计算

> 现代操作系统往往使用组合方式实现多线程，即线程创建完全在用户空间中完成，并且一个应用程序中的**多个用户级线程被映射到一些内核级线程上**

### 协程

协程是一种**用户态的轻量级线程**

- 协程的调度完全由用户控制

- **协程拥有自己的寄存器上下文和栈**

  - 协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈
  - 直接操作栈基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快
  - 协程能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态

- 一个线程可以拥有多个协程，一个进程也可以拥有多个协程

- 线程进程都是同步机制，而协程则是异步

  

### 进程和线程

`Process`和`Thread`

- 进程是系统进行**资源分配**的基本单位，线程是**CPU调度**的基本单位
- 线程依赖于进程而存在，一个进程至少有一个线程
- 进程有自己的独立地址空间，**线程共享所属进程的地址空间**
- 进程是拥有系统资源的一个独立单位，而线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源（如程序计数器PC，寄存器REG和**栈**），和其他线程共享本进程的相关资源（如内存，堆，I/O，CPU等）
- 在进程切换时，涉及到整个**当前进程CPU环境**的保存环境的设置以及新被调度运行的CPU环境的设置，而线程切换只需保存和设置少量的寄存器的内容，并不涉及存储器管理方面的操作
  - **进程切换的开销远大于线程切换的开销**
- 线程之间的通信更方便，同一进程下的线程**共享全局变量等数据**，而进程之间的通信需要以进程间通信（IPC）的方式进行
- **多线程程序**只要有一个线程崩溃，整个程序就崩溃了，但多进程程序中一个进程崩溃并不会对其它进程造成影响，因为**进程有自己的独立地址空间**
  - 多进程更加健壮

> 进程切换开销太大，因此引入线程



## 进程通信

**进程间通信是实现进程同步的手段**

- 共享内存（Shared Memory）
- 消息队列（MQ：Message Queue）
- 管道（Pipe）
- 命名管道
- 信号（Signal）
- 信号量（Semaphore）
- 套接字（Socket）

### 信号

信号是Linux系统**响应某些条件而产生的一个事件**，是一种**更高层的**软件形式的异常，一个信号代表了一个消息

- 信号的作用是用来**通知进程**发生了某种系统事件，是一种**订阅-发布**的模式

- 由操作系统事先定义，接收到该信号的进程可以采取自定义的行为

**信号来源**

- 硬件来源：如按下`CTRL+C`、除0、非法内存访问等
- 软件来源：如`kill`命令、Alarm Clock超时、当Reader中止之后又向管道写数据等

> `kill -9`

**一般的信号是都是由一个错误产生的**

- 除数为0时会引发0号中断，即除零异常，这是一个硬件级中断，会导致陷入内核，执行操作系统预定义在 IDT中的中断处理程序

- 操作系统**处理这个异常的方法就是向进程发送一个信号** `SIGFPE`。如果进程设置了相应的signal handler，就执行进程的处理方法。否则就执行操作系统的默认操作（一般这种信号的默认操作是杀死进程）

> 操作系统会**将这些硬件异常包装成信号发送给进程**，如果进程不处理这几个异常信号，默认的行为就是挂掉

**进程发送信号**

- 操作系统提供发送信号的**系统调用**
- 发送信号时，必须指明发送的目标进程的PID
  - 一般用在具有**亲缘关系的进程之间**
- 系统调用会**将信号放到目标进程的信号队列中**
  - 如果目标进程未处于执行状态，则该**信号就由内核保存**起来，直到该进程恢复执行并传递给它为止
  - 如果一个**信号被进程设置为阻塞**，则该信号的传递被延迟，直到其阻塞被取消时才被传递给进程

**进程接收信号**

- 每个进程有一个信号队列，存放**其他进程发送给它，等待它处理的信号**
- 进程在执行过程中的**特定时刻检查并处理自己的信号队列**
  - 如从系统空间返回到用户空间之前

**用户进程对信号的处理**

- 处理信号

  - 定义**信号处理函数**，当信号发生时执行相应的处理函数
- 忽略信号

  - 当不希望接收到的信号对进程的执行产生影响时，可以忽略该信号，不对信号进程作任何处理，从而让进程继续执行时
- 不处理也不忽略

  - 执行默认操作，linux系统对每种信号都规定了默认操作
- 有些信号用户进程是无法处理也无法忽略的（如`SIGSTOP`、`SIGKILL` 等）



### 管道

管道是一种**半双工的通信方式**，数据只能**单向流动**

- 上游进程往管道中写入数据，下游进程从管道中接收数据
- 如果想实现双方通信，那么需要建立两个管道

- 管道发送的内容是**没有格式的字节流**，适合于**传输大量信息**

**管道同步**

操作系统会保证**读写进程的同步**

- 管道就相当于一个文件，**同一时刻只能有一个进程访问**
  - 下游进程或者上游进程需要**等另一方释放锁后才能操作管道**。
    - 当管道为空时，下游进程读阻塞
    - 当管道满时，上游进程写阻塞

> 在 Linux Shell 中经常使用管道操作符`|`来表示两个命令之间的数据通信
>
> - 管道操作符`|`的内部实现就是 Linux 的管道接口
> - 由管道操作符`|`分割的**每个命令是独立的进程**，各个进程的标准输出`STDOUT`，会作为下一个进程的标准输入`STDIN`

**创建管道**

Linux管道包含**匿名管道**和**命名管道**，匿名管道只能用在**亲缘进程**中，管道文件信息保存在内存里

- 通过 `pipe()` 系统调用来创建并打开一个**匿名管道**
- 匿名管道只能用于具有**亲缘关系的进程**之间
  - 父子进程或兄弟进程

> 当管道不再被任何进程使用时会自动消失

**实现管道**

- **管道就是一个文件**
  - 一种**只存在于内存中**的特殊的文件系统
- 在Linux中管道借助了**文件系统的File结构**实现
  - 父进程使用File结构保存向管道写入数据的例程地址，子进程保存从管道读出数据的例程地址（单向流动，亲缘进程）
- 管道是由内核管理的一个**环形的数据结构缓冲区**，以便管道可以被循环利用（循环队列）



### 命名管道

`FIFO`

- 命名管道是一种**全双工的通信方式**
- 命名管道可用于没有亲缘的进程间
- 通过 `mknode()` 系统调用或者 `mkfifo()` 函数创建命名管道
  - 任何有访问权的进程都可以**通过文件名将其打开和进行读写**，而不局限于亲缘进程
- 创建命名管道时会**在磁盘中创建一个索引节点**，命名管道的名字就相当于**索引节点的文件名**
  - 索引节点设置了**进程的访问权限**，但是没有数据块
- 命名管道实质上也是通过**缓冲区**来实现数据传输
  - 有访问权限的进程可以通过磁盘的索引节点来读写这块缓冲区
- 当不再被任何进程使用时命名管道在内存中释放，但磁盘节点仍然存在

### 信号量

信号量是一种特殊的变量，**对它的操作都是原子的**

- 在Linux系统中，二进制信号量又称**互斥锁**（`Mutex`）
  - 可以用于实现进程或线程的互斥和同步

- 信号量在底层的实现是**通过硬件提供的原子指令**（如 `Test And Set`、`Compare And Swap` 等）

> V：`signal()`，P：`wait()`
>
> - `V(S)`：如果有其他进程因等待S而被挂起，就让它恢复运行，否则S加 1
>
> - `P(S)`：如果S<=0，则挂起进程，否则S减1



### 共享内存

允许多个进程共享同一段物理内存

- **不同进程可以将同一段共享内存映射到自己的地址空间，然后像访问正常内存一样访问它**
  - 共享内存区只会驻留在创建它的进程地址空间内。
  - 不同进程可以通过向共享内存端读写数据来交换信息

**优点**

**简单且高效**：访问共享内存区域和访问进程独有的内存区域一样快

- 不需要系统调用，不涉及**用户态到内核态的转换**
  - 只在建立共享内存区域时需要系统调用
  - 建立共享内存后所有访问都可作为**常规内存访问**，无需借助内核
- 不需要对数据不必要的复制

**缺点**

**存在并发问题**：有可能**多个进程同时修改同一块内存**

- 因此共享内存一般与信号量结合使用

> Linux2.2.x内核支持多种共享内存方式
>
> - `mmap()`系统调用
>   - 将**普通文件**映射到进程的地址空间，然后可以像访问普通内存一样对文件进行访问，不必再调用`read()`，`write()`等操作
>   - `mmap()`不是专门用来共享内存的，但是多个进程可以通过`mmap()`映射同一个普通文件来实现共享内存
> - 系统V
>   - 通过**映射特殊文件系统shm中的文件**实现进程间的共享内存
>   - 通过`shmget`可以创建或获得共享内存的标识符，取得共享内存标识符后，通过`shmat`将这个内存区映射到本进程的虚拟地址空间



### 消息队列

消息队列是一个消息的链表，**保存在内核中**

- 消息队列中的**每个消息都是一个数据块，具有特定的格式**
- 操作系统中可以存在多个消息队列，每个消息队列有唯一的`key`，称为消息队列标识符
  - 操作系统提供创建消息队列、取消息、发消息等系统调用
  - 操作系统负责**读写同步**
    - 若消息队列已满，则写消息进程排队等待
    - 若取消息进程没有找到需要的消息，则在等待队列中寻找
- 消息队列是**异步的**
  - 消息队列允许一个或多个进程向它写入和读取消息
  - 消息的发送者和接收者**不需要同时与消息队列交互**，消息会保存在队列中，直到接收者取消息

**优点**

消息队列克服了**信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限**等缺点

- 和信号相比，消息队列能够传递更多的信息
- 与管道相比
  - 消息队列提供了**有格式的数据**
  - 取消息进程**可以选择接收特定类型的消息**，而不像管道默认全部接收

**缺点**

- 消息队列仍然有大小限制
- 消息队列是**异步的**，所以接收者必须**轮询**消息队列才能收到最近的消息



### 套接字

**不同的计算机的进程**之间通过socket通信

- 也可用于同一台计算机的不同进程

- 需要通信的进程之间首先要**各自创建一个socket**，声明自己接收来自端口的数据
  - 内容包括主机地址与端口号（`ip:port`）
- 进程通过socket把消息发送到网络层中，网络层通过主机地址将其发到目的主机，**目的主机通过端口号发给对应进程**

- 操作系统提供创建socket、发送、接收的系统调用
  - 为每个socket设置**发送缓冲区**，**接收缓冲区**

## 进程调度

线程是CPU调度的最小单位

**批处理系统**

- 先来先服务：first-come first-serverd（FCFS）
  - 非抢占式，开销小，无饥饿问题

- 短作业优先：shortest job first（SJF）
  - 非抢占式，开销可能较大，可能导致饥饿问题

- 最短剩余时间优先：shortest remaining time next（SRTN）
  - 短作业优先的抢占式版本
- 最高响应比优先：Highest Response Ratio Next（HRRN）

**交互式系统**

- 时间片轮转：Round Robin
  - 抢占式，开销小，无饥饿问题
- 优先级调度算法：Priority
- 多级反馈队列调度算法：Multilevel Feedback Queue

## 进程分类

- 僵尸进程：停止运行
- 孤儿进程：正在运行
- 守护进程：正在运行

### 僵尸进程

僵尸进程是指**终止但还未被回收**的进程

- 当一个进程由于某种原因终止时，**内核并不是立即把它从系统中清除**。
- 进程会保持在一种**已终止**的状态中，直到被它的**父进程回收**。当父进程回收已终止的子进程时，内核会抛弃已终止的进程，此时该进程就不存在了
- 如果子进程退出，而父进程并没有调用 `wait()` 或 `waitpid()` 来回收，那么就会产生僵尸进程
- 僵尸进程是一个已经死亡的进程，但是其**进程描述符**仍然保存在系统的进程表中
- 如果杀死父进程，僵尸进程就会变成孤儿进程，由`Init`进程接管并处理

**危害**

- 占用进程号
  - 系统所能使用的进程号是有限的，可能导致不能产生新的进程
- 占用一定的内存

### 孤儿进程

如果某个进程的**父进程先结束了**，那么它的子进程会成为孤儿进程

- 每个进程结束的时候，系统都会扫描是否存在子进程，如果有则用`Init`进程（`pid = 1`）接管，并由`Init`进程调用 `wait` 等待其结束，完成状态收集工作。
- **孤儿进程不会对系统造成危害**

### 守护进程

守护进程：daemon

- 一种**在后台执行**的电脑程序，此类程序会被以进程的形式初始化

## 进程同步

- **临界资源**：可以为若干线程所共享，但一次只能为一个线程所利用的**资源**
- **临界区**：访问临界资源的**程序片段**，当有线程进入临界区时，其他线程或是进程必须等待
  - 有一些同步机制必须在临界区的进入点与离开点实现，以确保这些共用资源是被互斥获得使用

## 锁

Java中的锁类型

- 公平锁/非公平锁
- 可重入锁
- 独享锁/共享锁
- 互斥锁/读写锁
- 乐观锁/悲观锁
- 分段锁
- 偏向锁/轻量级锁/重量级锁
- 自旋锁

### 自旋锁

`Spinlock`

**自旋**：持续观察一个变量直到它发生改变的过程

**自旋锁**：一个线程在获取锁的时候，如果锁已经被其它线程获取，那么**该线程将循环等待，然后不断的判断锁是否能够被成功获取**，直到获取到锁才会退出循环

自旋锁通过一个**忙标志**`flag`表示锁是否被占用

- 当`flag = 0`时表示锁空闲
- 当一个线程成功将`flag`从`0`变为`1`时，表示该线程获得锁
  - 线程在 `while` 循环中尝试通过TAS（Test And Set）等硬件**原子指令**获取锁

**优点**

- 不会使线程状态发生切换，这就避免了操作系统**重新调度**和**上下文切换**的开销
  - 操作系统内核经常使用自旋锁

> 自旋锁的性能在**多处理器**的场景下性能要比单处理器更好（假设线程均匀分布在多个 CPU 上）
>
> - 因为如果当前线程持有锁并很快释放，那么其他线程很可能在自旋的时候就能直接获取到这个锁，这样不会浪费整个时钟周期

**缺点**

- 在单处理器的场景下，如果锁已经被另一个线程持有，那么当前线程**在尝试加锁时需要将整个时间片空转完**，除非发生上下文切换，否则它是不可能获取到锁的
- 自旋锁可能会导致**饥饿**

因此，自旋锁适用于线程**持有锁的时间很短**的场景。线程持有锁的时间越长，则持有锁的线程被 OS 调度程序中断的风险越大，其他线程空转浪费时间片的概率也越大。





### 互斥锁

`Mutex`

互斥锁需要操作系统的帮助。当一个线程访问其他线程持有的锁时，会被 OS 调度为阻塞状态（休眠），直到锁被释放后，再唤醒一个休眠的线程。

互斥锁的开销主要体现在线程的**重新调度**和**上下文切换**上，获取锁的开销是比较大的。因此 mutex 适用于线程**持有锁时间比较长**的场景。

如果线程持有锁的时间比较短，使用mutex会因为频繁的线程切换而导致效率变差，大部分时间都花在了用户态到内核态的切换（系统调用）、重新调度（移到阻塞队列）和上下文切换（线程切换）上了，还不如使用自旋锁的效率高。

### 死锁

`DeadLock`

**两个或多个进程**才有可能出现死锁

- 一个进程持有某种资源而又等待其它进程释放它们现在保持着的资源，若无外力作用它们都将无法推进下去，称这一组进程产生了死锁

> 相互等待，无限期阻塞的一种状态

**死锁产生的四个必要条件**（有一个条件不成立都不会产生死锁）

- 互斥条件：一个资源一次只能被一个进程使用
- 请求与保持条件：一个进程因请求资源而阻塞时，**对已获得资源保持不放**
- 不剥夺条件：进程获得的资源，在未完全使用完之前，**不能强行剥夺**
- 循环等待条件：若干进程之间形成一种头尾相接的**环形等待资源关系**

**解决死锁的基本方法**

- 破坏四个必要条件
  - **互斥条件无法被破坏**
  - 破坏请求与保持条件实现起来困难，会降低系统性能
  - 破坏不剥夺条件：需要采取**资源预先分配策略**
    - 一次性地向系统申请它所需要的全部资源。如果某个进程所需的全部资源得不到满足，则不分配任何资源，此进程暂不运行。只有当系统能够满足当前进程的全部资源需求时，才一次性地将所申请的资源全部分配给该进程
    - 资源利用率低，降低了进程的并发性
  - 破坏循环等待条件：需要采取**资源有序分配策略**
    - 把资源事先分类编号，按号分配，所有进程对资源的请求必须严格按资源序号递增的顺序提出，进程占用了小号资源，才能申请大号资源，就不会产生环路
- 避免死锁发生
  - 动态地检测资源分配状态，以确保系统处于安全状态，只有处于安全状态时才会进行资源的分配
  - 银行家算法：找出安全分配的进程序列
- 解决发生的死锁
  - 利用抢占：挂起某些进程，并抢占它的资源。但要防止某些进程被长时间挂起而处于饥饿状态
  - 利用回滚：让某些进程回退到足以解除死锁的地步，进程回退时自愿释放资源。要求系统保持进程的历史信息，设置还原点
  - 利用杀死进程：强制杀死某些进程直到死锁解除为止，可以按照优先级进行



## 文件

- 文件描述符在形式上是一个非负整数，**它是一个索引值**，指向内核为每一个进程所维护的该**进程打开文件的记录表**
- 当程序打开一个现有文件或者创建一个新文件时，**内核向进程返回一个文件描述符**，内核通过文件描述符来访问文件

## IO模型

指网络IO，**CPU处理数据的速度远远大于IO准备数据的速度**

> 网络IO的耗时跟磁盘IO差不多

**IO模型**

- **同步IO模型**
  - 阻塞IO（BIO：Blocking IO）
  - 非阻塞IO（NIO：Non-Blocking IO）
  - **IO多路复用**
  - 信号驱动IO
- **异步IO模型**
  - AIO
  - IOCP

### Socket

- Socket是在应用层和传输层中间的**抽象层**
  - 它把传输层（TCP/UDP）的复杂操作抽象成一些简单的接口，**供应用层调用实现进程在网络中的通信**

- Socket起源于UNIX，**在Unix中一切皆文件**，在内核中Socket也是以文件的形式存在的，有对应的文件描述符

  - Socket是一种**打开—读/写—关闭**模式的实现

  - 服务器和客户端各自维护一个**文件**，在建立连接打开后，可以向文件写入内容供对方读取或者读取对方内容，通讯结束时关闭文件

> Socket一般被翻译为套接字，其实它在英文的含义为插座，Socket就像一个插座，负责连通两端，进行点对点通信
>
> - 端口就像插座上的孔，端口不能同时被其他进程占用
> - 建立连接就像把插头插在这个插座上，创建一个Socket实例
> - 开始监听后，这个插座就时刻监听着消息的传入，谁拨通IP和端口，就接通谁

- 操作系统**内核通过网卡读取网络数据**，将网络数据存储在内存中
- 计算机中会运行不同的网络程序，这些不同的网络程序可能对应系统中的不同的进程
- Socket设计的目的和想要解决的问题就是把网卡中的网络数据识别出来是给哪个进程的，并持续和稳定地给到对应的应用进程

#### 处理请求

1. 内核通过网卡将数据copy到内核缓冲区
   1. 内核会通过**DMA技术**控制IO是否阻塞
2. 用户进程再将数据从内核缓冲区copy到用户空间处理
   1. 通过**mmap内存映射**控制IO是否同步
      1. mmap内存映射是指**用户空间和内核空间映射同一块内存空间**
      2. 从而省略将数据从内核缓冲区拷贝到用户空间的操作，**用户空间通过映射直接操作内核缓冲区的数据**

#### Socket通信过程

1. 服务端首先调用`socket()`函数，**创建一个Socket**
   1. 其中网络协议为IPv4，传输协议为TCP 
2. 服务端调用`bind()`函数，**给这个Socket绑定一个IP地址和端口**
   1. 绑定端口：内核收到TCP报文后通过TCP头里面的端口号找到应用程序，然后传递数据
   2. 绑定IP地址：一台机器可以有多个网卡，**每个网卡对应一个IP地址**，绑定一个网卡的IP地址后，内核才能收到该网卡上的数据
3. 服务端调用`listen()`函数**对这个Socket进行监听**
   1. 可以通过`netstat`命令查看对应的端口号是否有被监听来**判断服务器中的某个应用程序有没有启动**
4. 服务端进入了监听状态后，通过调用`accept()`函数**从内核获取来自客户端的连接请求**
   1. 如果没有客户端请求连接，则**会阻塞等待客户端连接的到来**
5. 客户端也调用`socket()`函数，**创建一个Socket**，然后调用`connect()`函数向服务端发起连接
   1. **开始TCP三次握手**
   2. `connect()`函数的参数会指明服务端的IP地址和端口号

6. 服务端的`accept()`函数在获取连接后会**返回用于传输的Socket的文件描述符，后续用这个Socket来传输数据**
   1. **监听的Socket和真正用来传数据的Socket是两个不同的Socket**
7. 客户端和服务端使用`write()`函数和`read()`函数向服务端请求和发送数据

#### Socket队列

在TCP连接的过程中，**服务端内核实际上为每个Socket维护了两个队列**

- **TCP半连接队列**：还没完全建立连接的队列，这个队列都是没有完成三次握手的连接
- **TCP全连接队列**：已经建立连接的队列，这个队列都是完成了三次握手的连接

当**TCP全连接队列**不为空后，服务端的`accept()`函数会从内核中的TCP全连接队列里拿出一个已经**完成连接的Socket**返回应用程序，后续数据传输都用这个Socket

#### Accept函数在TCP三次握手的那一部分开始，哪一部分返回

服务端调用`listen()`函数进行监听，然后使用`accept()`函数等待请求（**阻塞等待**），客户端通过`connect()`函数发起请求

- 第一次握手：客户端发送syn包到服务器
  - `accept()`函数被阻塞
  - `connect()`函数被阻塞
- 第二次握手：服务器收到syn包，确认客户端的SYN，然后发送ack包到客户端
  - `accept()`函数被阻塞
  - `connect()`函数被阻塞
- 第三次握手：客户端收到syn包和ack包，发送ack包到服务器
  - `accept()`函数被阻塞
  - `connect()`函数完成建立连接的功能

服务端收到ack包，**三次握手完成，TCP连接建立，服务器调用`accept()`函数获得返回的连接**

#### 服务器单机理论最大能连接的客户端数目

TCP连接由四元组唯一确认：**本机IP, 本机端口, 对方IP, 对方端口**

- 服务器的IP和端口是固定的，四元组只有对方IP和端口会变化，所以**最大TCP连接数 = 客户端IP数×客户端端口数**
- 对于IPv4，客户端的IP数最多为2<sup>32</sup>次方，客户端的端口数最多为2<sup>16</sup>次方，所以理论上**服务端单机的最大TCP连接数约为2<sup>48</sup>**
- 但Socket实际上是一个文件，会对应一个文件描述符。在Linux下，**单个进程打开的文件描述符数是有限制的**，没有经过修改的值一般都是1024
- **每个TCP连接在内核中都有对应的数据结构**，即每个连接都是会占用一定内存

### 同步IO

上述Socket调用使用的是**同步阻塞（BIO）的方式，只能一对一通信**

- 当服务端在还没处理完一个客户端的网络I/O 时，或者读写操作发生阻塞时，其他客户端是无法与服务端连接的

> 进程从调用到返回这段时间内都是被阻塞的称为阻塞IO，否则就是非阻塞IO

#### 同步阻塞IO

- 应用进程调用`recv()`系统调用，**等待数据时一直阻塞**，直到内核数据拷贝到用户空间

#### 同步非阻塞IO

- 应用进程一直**轮询调用`recv()`系统调用查看内核缓冲区的数据是否准备好**，内核立即给予答复，**进程不会阻塞**
- 如果内核通知数据还未准备好，则应用进程接着轮询
- 系统调用带来的来回的**用户态和内核态的切换**导致成本几何上升

#### IO多路复用

`IO Multiplexing`

只使用一个进程来处理多个网络连接（维护多个Socket），就是**I/O多路复用**技术，也叫**事件驱动IO**

- 由OS提供`select()`，`poll()`，`epoll()`三个系统调用函数**不断轮询进程所负责的所有Socket**
  - 即单个处理的线程只受阻于select()调用，由内核去轮询查看某一个Socket是否有数据到达
- 当某个Socket有数据到达了（数据状态准备就绪），就通知用户进程处理数据

> Recv 只能监视单个 Socket，为每个请求连接分配一个进程/线程的方式开销太大

**思想**

- 类似于一个CPU并发多个进程，用切分时间片的方式**让多个请求复用一个进程**
- 只需要一个或几个线程就可以完成**数据状态询问**的操作，当有数据准备就绪之后再分配对应的线程去读取数据



#### select/poll/epoll三者的区别

select和poll是线程不安全的，epoll是线程安全的

**select**

预先传入一个Socket列表，如果列表中的Socket都没有数据，挂起进程，直到有一个Socket收到数据，唤醒进程

- 将所有Socket的文件描述符放入一个集合中
  - 集合大小有限制，32位机默认是1024（64位：2048）
- 调用`select()`时，**将Socket集合从用户空间拷贝到内核空间**，由内核根据文件描述符的就绪状态修改该集合的内容
  - 每次都要复制，开销大
- 采用水平触发机制（LT，Level Trigger）
- `select()`返回后，**进程需要通过遍历这个集合找到就绪的文件描述符**
  - 当文件描述符的数量增加时，轮询的方式效率较低

**poll**

- 和`select()`的区别在于文件描述符的存储方式不同
  - **poll采用链表的方式存储，没有最大存储数量的限制**


**epoll**

- **通过内核和用户空间共享内存**，避免了不断复制的问题
- 支持的同时连接数上限提高（1G左右的内存支持10W左右的连接数）
- **文件描述符就绪时采用回调机制，避免了轮询**
  - 回调函数**将就绪的描述符添加到一个链表中**，执行`epoll_wait`时，返回这个链表
- 支持水平触发和边缘触发（ET，Edge Trigger），采用边缘触发机制时，**只有活跃的文件描述符才会触发回调函数**
- epoll这个系统调用对外部来说，是一个同步的接口，必须等待操作系统返回值

> epoll通过红黑树来组织所有监控的socket对象，实现高效的查找，删除和添加
>
> epoll系统调用依旧是同步的，必须等待操作系统返回值
>
> 用户应用不用自己去进行遍历查询



## Copy-on-write

`Copy-on-write`：COW，写时复制，也称为隐式共享（implicit sharing）

- **将复制操作推迟到第一次写入时**进行
  - 在创建一个新副本时，不会立即复制资源，而是**共享原始副本的资源**，当修改时再执行复制操作
- 通过这种方式共享资源，可以显著减少创建副本时的开销，以及节省资源

> 主要在很多情况下根本不需要复制，就节省了大量时间，充分使用了稀有的物理内存
>
> 但如果在子进程存在期间发生了大量写操作，那么会频繁地产生页面错误，不断陷入内核来复制页面，反而会降低效率

### 为什么需要COW

当通过`fork()`系统调用来创建一个子进程时，**操作系统需要将父进程虚拟内存空间中的大部分内容全部复制到子进程中**

- 主要是数据段、堆、栈；代码段共享
- 这个操作不仅非常耗时，而且会浪费大量物理内存
- 如果程序在进程复制后立刻使用`exec`加载新程序，那么负面效应会更严重，相当于之前进行的**复制操作是完全多余的**

因此引入了COW技术

- 内核不会复制进程的整个地址空间，而是**只复制进程的页表**，`fork()`之后的**父子进程的地址空间指向同样的物理内存页**
- 不同进程的内存空间应当是**私有**的，假如所有进程都只读取其内存页，那么就可以**继续共享物理内存中的同一个副本**。但是只要有一个进程试图写入共享区域的某个页面，那么就会**为这个进程创建该页面的一个新副本**

### 原理

- `fork()`之后，内核会**把父进程的所有内存页都标记为只读**。一旦其中一个进程尝试写入某个内存页，就会**触发一个保护故障（缺页异常）**，此时会陷入内核
- **内核将拦截写入**，并为尝试写入的进程**创建这个页面的一个新副本，恢复这个页面的可写权限**，然后重新执行这个写操作，这时就可以正常执行了

- **内核会保留每个内存页面的引用数**。每次复制某个页面后，该页面的引用数减少一。如果该页面只有一个引用，就可以跳过分配，直接修改

> 这种分配过程对于进程来说是透明的，能够确保一个进程的内存更改在另一进程中不可见

### 应用

Redis的持久化机制中，如果采用`bgsave`或者`bgrewriteaof`命令，就会 `fork()`一个子进程来**将数据存到磁盘中**

- Redis的读取操作多，这种情况下使用COW可以减少`fork()`操作的阻塞时间

## 零拷贝



# Linux

## 查看CPU使用率、内存、磁盘、进程端口

### top

`top`：显示当前系统**正在执行的进程**的相关信息，包括进程ID、内存占用率、CPU占用率等

- 进程数
  - running，sleeping

- 进程占用CPU的使用
- 进程使用的物理内存和总内存的百分比

`load average`字段：CPU使用率

- 由逗号分割的3列数字分别代表了最近1分钟，5分钟，15分钟**CPU的平均负载**情况

- 单核CPU的话，1.00就表示CPU已经满负荷了。多核CPU的话，load average达到CPU的核数即说明已经满负荷了。多颗物理CPU，则load average达到所有物理CPU的总核数时，说明已经满负荷了。

### free

`free`：简单查看内存状况

### df

disk free

`df -h`：查看磁盘使用情况

- 文件系统
- 容量使用
- 挂载点

### ps

process status

`ps –ef`：查看进程的状态

- UID
- PID

> `ps -ef | grep tomcat`

## 如何查看某个端口是否被占用

`netstat -anp | grep  port`

- 主要看监控状态：`LISTEN`表示已经被占用，最后一列显示占用的服务，对应具体端口号
  - 显示`LISTENING`并不表示端口被占用

`netstat -nultp`

- 查看当前**所有已经使用**的端口情况，状态都是`LISTEN`



# Docker

Docker是一个**容器引擎**，是应用程序与系统之间的隔离层

- 通常**应用程序对安装的系统环境**会有要求，如果服务器很多，部署时系统环境的配置工作是非常繁琐的


- Docker让应用程序不必再关心主机环境，各个应用安装在Docker镜像里，**Docker引擎负责运行包裹了应用程序的docker镜像**

**系统沙箱**

- **虚拟机方案**
  - 需要Hypervisor实现**硬件资源虚拟化**，每个app都有独立的Guest OS
- **容器方案**（linux kernel）
  - **只有一个HostOS**，容器上的app**直接使用实际物理机的硬件资源**

## Docker原理

- 通过namespaces隔离了进程树，网络接口和挂载点，实现进程之间的通信
- 通过CGroups隔离了CPU，内存，磁盘I/O和网络带宽
- 通过联合文件系统构成docker文件，镜像等

**容器也拥有独立的文件系统**

1. 宿主机上多个容器运行相同的系统，这些**系统里大部分文件内容都是相同的**
2. 为了节省资源，**将相同的内容和不同的内容隔开，分成只读层和可写层**
3. 然后就可以挂载同一个只读层，再挂载不同的可写层上
   1. 只读层（相同）：镜像
   2. 可写层（不同）：容器

## Docker架构

C-S架构，分为2部分

- **docker Client**：输入各种docker命令
- **docker Server**：有一个**Daemon后台进程**，负责和Client通信，操作容器

## 虚拟机VM和Docker的区别

**Docker容器不是虚拟机**

容器和虚拟机的主要区别是

- 容器提供了**基于进程的隔离**
- 虚拟机提供了资源的完全隔离

容器使用宿主操作系统的内核，而虚拟机使用独立的内核

Docker使用的是Linux容器（LinuX Container, 简称**LXC**），跟其宿主运行同样的操作系统，**多个容器之间是共用同一套操作系统资源的**

> 虚拟机的启动比容器慢很多，虚拟机可能要一分钟来启动，但是容器只需要几秒甚至不到一秒
>

# 数据结构

## 树

**应用场景**

- xml，html等的解析器
- 文件系统的目录结构
  - 文件目录树的起点是根目录
  - Linux文件系统中每一个文件在此目录树中的文件名都是独一无二的，因为其包含从根目录开始的完整路径
- MySQL数据库索引
  - B+树
- 路由协议
  - STP生成树协议，确保网络中没有环路
  - SPF最优树协议，确保网络中没有环路，还保障网络路径最优（网络路径代价最小）
- 数据文件压缩
  - 哈夫曼树广泛地用于数据文件压缩，十分有效的编码方法
- linux中进程的调度
  - 红黑树

## 链表

- Linux文件系统
  - 索引链接磁盘
- Git
  - 每次commit都是创建一个node，node包含了删减后的新文件，然后node指向前一个commit的node
- C语言标准库中`malloc()`函数
  - 内存块的分布是离散的，将各个内存块以链表的形式连接，组成空闲链表
  - 在用户使用`malloc()`函数申请内存时，遍历链表找到一个足够大的，能够满足用户需求的用户的大内存快，将它划分出一个与用户申请的内存相同大小的块，返回块地址
- 数据库
  - 字典使用用链表来解决冲突

# 设计模式

## 单例模式

单例模式是指**在内存中只会创建且仅创建一次对象的设计模式**

- 确保某个类只有一个实例

- 在程序中多次使用同一个对象且作用相同时，为了**防止频繁地创建对象使得内存飙升**
- 单例模式可以让程序仅在内存中创建一个对象，**让所有需要调用的地方都共享这一单例对象**

**单例模式类型**

- 饿汉式：在**类加载**时已经创建好该单例对象，等待被程序使用
- 懒汉式：在**真正需要使用**对象时才去创建该单例类对象

### 饿汉式

饿汉式**在类加载时已经创建好对象**，在程序**调用时直接返回该单例对象**

- 类加载时会在堆内存中创建一个对象，当类消亡时对象也随之消亡了
- 如果对内存要求不高使用饿汉式，简单不易出错，且**没有任何并发安全和性能问题**
  - 如果从未使用过这个实例，会造成内存的浪费

```java
public class Singleton{
    
    private static final Singleton singleton = new Singleton();
    private Singleton(){}
    
    public static Singleton getInstance() {
        return singleton;
    }
}
```



### 懒汉式

懒汉式创建对象的方法是**在程序使用对象前先判断该对象是否已经实例化**（判空），若已实例化直接返回该类对象，**否则执行实例化操作再返回**

- 核心方法不加锁会存在线程安全问题

```java
public class Singleton {
    private static Singleton singleton;
    private Singleton(){}
    // 线程不安全
    public static Singleton getInstance() {
        if (singleton == null) {
            singleton = new Singleton();
        }
        return singleton;
    }
}
```

**加锁**

- 同步方法
  - 加锁后**每次去获取对象都需要先获取锁，并发性能非常地差**
  - 而且其实创建的方法只需要执行一次，后面直接返回对象即可
- 同步代码块
  - **双重检查**（Double-Check）
  - 线程安全，延迟加载，效率较高

```java

// 加锁，同步方法
public static synchronized Singleton getInstance() {
    if (singleton == null) {
        singleton = new Singleton();
    }
    return singleton;
}

// 加锁，同步代码块
public static Singleton getInstance() {
    if (singleton == null) {
        synchronized(Singleton.class) {
            // 再次判断，防止多个线程同时进入上一个null判断
            if(singleton == null) {
                singleton = new Singleton();
            }
        }
    }
    return singleton;
}
```

**优化性能**

- 如果没有实例化对象，则**加锁创建对象**
- 如果已经实例化了，则**不需要再获取锁而是直接获取实例**

> 需要两次判空，且对类对象加锁

```java
public class Singleton {
    
    private static Singleton singleton;
    private Singleton(){}
    
    public static Singleton getInstance() {
        if (singleton == null) {
            // 线程获得锁才可以进行实例化
            synchronized(Singleton.class) { 
                if (singleton == null) { 
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
```

**指令重排**

JVM在保证最终结果正确的情况下，**可以不按照程序编码的顺序执行语句，尽可能提高程序的性能**

使用`volatile`关键字修饰的变量，**可以保证其指令执行的顺序与程序指明的顺序一致**，不会发生顺序变换

> `volatile`的作用仅仅是阻止指令重排序, 不涉及可见性问题，可见性已经由`synchronized`来保证
>
> `synchronized`也有阻止重排序的功能，但是由monitor实现，monitorenter和monitorexit之间的指令仍可能被重排序

```java
public class Singleton {
    
    // 防止指令重排
    private static volatile Singleton singleton;
    private Singleton(){}
    
    public static Singleton getInstance() {
        if (singleton == null) {
            // 线程获得锁才可以进行实例化
            synchronized(Singleton.class) { 
                if (singleton == null) { 
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
```

